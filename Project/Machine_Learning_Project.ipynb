{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JmTL-5ZyQYiY",
        "outputId": "39ad1755-0a91-405e-a9c8-cdee03f3c5e1"
      },
      "outputs": [],
      "source": [
        "pip install ucimlrepo numpy pandas matplotlib scikit-learn seaborn autograd torch tabulate xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpMpDCZBQ-YY",
        "outputId": "e75bc3f1-2568-41c4-d123-7593bcd9a099"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "    # fetch dataset\n",
        "    support2 = fetch_ucirepo(id=880)\n",
        "\n",
        "    # data (as pandas dataframes)\n",
        "    X = support2.data.features\n",
        "    y = support2.data.targets\n",
        "\n",
        "    # metadata\n",
        "    print(support2.metadata)\n",
        "\n",
        "    # variable information\n",
        "    print(support2.variables)\n",
        "except ConnectionError as e:\n",
        "    import pandas as pd\n",
        "    print(\"Unable to fetch dataset. Trying to load from local file.\")\n",
        "    support2 = pd.read_csv(\"data.csv\")\n",
        "    X = support2.iloc[:,:-3]\n",
        "    y = support2.iloc[:,-3:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# support2.variables.to_csv(\"support2_codebook.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 808
        },
        "id": "Akp6EawHRg8K",
        "outputId": "c4bd3000-8071-41c6-dc6f-b42e68fac0e2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "data = pd.concat([X,y],axis=1)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "missed_target_data = data[data['sfdm2'].isnull()]\n",
        "data = data.dropna(subset=['sfdm2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "def dataUnderstanding(data):\n",
        "  # Number of the rows and columns\n",
        "  rows, columns = data.shape\n",
        "  print('-' * 50)\n",
        "  print(f\"Number of Rows:{rows} | Number of Columns:{columns}\")\n",
        "  print('-' * 50)\n",
        "  print()\n",
        "\n",
        "  # List of columns\n",
        "  columns_df = pd.DataFrame(data.columns, columns=[\"Column Names\"])\n",
        "  print(\"List of Features in the dataset:\")\n",
        "  print(tabulate(columns_df, headers='keys', tablefmt='psql', showindex=False))\n",
        "  print('-' * 50)\n",
        "  print()\n",
        "\n",
        "  # Print Data type\n",
        "  print(\"Summarized basic information:\\n\")\n",
        "  data.info()\n",
        "  print('-' * 50)\n",
        "  print()\n",
        "\n",
        "  # printing all the numerical datatype columns\n",
        "  numerical_columns = data.select_dtypes(include=['number']).columns.tolist()\n",
        "  print(f\"Printing all the numerical columns --> {numerical_columns}\")\n",
        "  print()\n",
        "  # printing all the object datatype columns\n",
        "  object_columns = data.select_dtypes(include=['object']).columns.tolist()\n",
        "  print(f\"Printing all the Object columns --> {object_columns}\")\n",
        "  print('-' * 50)\n",
        "  print()\n",
        "\n",
        "  # finding the missing values\n",
        "  print(f\"Finding the number of missing values in all the columns -->\\n\")\n",
        "  print(data.isna().sum())\n",
        "  print('-' * 50)\n",
        "\n",
        "\n",
        "dataUnderstanding(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJan6d3lTEm7"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEAUUiCcFQ0y"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moSW2jFWF5A-",
        "outputId": "b621b246-47d7-442f-8587-8f4678d9d47e"
      },
      "outputs": [],
      "source": [
        "missing_value_table = data.isnull().sum()\n",
        "missing_value_proportion = missing_value_table[missing_value_table>0].sort_values(ascending=False) / len(data)\n",
        "for i in missing_value_proportion.index:\n",
        "    print(\"{}: {:.2f}%\".format(i,missing_value_proportion[i]*100), f'dtype={data[i].dtype}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NC7ZLR-YMn1N",
        "outputId": "982a8c52-1753-49a6-99f2-08fd32c32de3"
      },
      "outputs": [],
      "source": [
        "X_pd = pd.DataFrame(X)\n",
        "y_pd = pd.DataFrame(y)\n",
        "X_pd.isnull().sum(), y_pd.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LZiQq3NZSqT0",
        "outputId": "b0e030c0-f679-4a49-d9c8-48686c0469c6"
      },
      "outputs": [],
      "source": [
        "for i in data.columns:\n",
        "  print(f'{i}:{data[i].dtype}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDlmd3qWT7f0",
        "outputId": "7525a5ac-ed1e-4c40-f70e-fec50f371b61"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.impute import KNNImputer, SimpleImputer\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "def fill_missing_values_simplified(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fill missing values in a DataFrame using KNNImputer for numerical columns\n",
        "    and SimpleImputer (most frequent) for categorical columns.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : pd.DataFrame\n",
        "        The input DataFrame containing missing values.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        The DataFrame with missing values filled.\n",
        "    \"\"\"\n",
        "    data_filled = data.copy()\n",
        "    \n",
        "    # Identify numerical and categorical columns\n",
        "    numerical_cols = data.select_dtypes(include=['number']).columns.tolist()\n",
        "    categorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    # Impute numerical columns with KNNImputer\n",
        "    if numerical_cols and data_filled[numerical_cols].isnull().sum().sum() > 0:\n",
        "        knn_imputer = KNNImputer(n_neighbors=13,weights='distance')\n",
        "        data_filled[numerical_cols] = knn_imputer.fit_transform(data_filled[numerical_cols])\n",
        "\n",
        "    # Impute categorical columns with the most frequent value\n",
        "    if categorical_cols and data_filled[categorical_cols].isnull().sum().sum() > 0:\n",
        "        simple_imputer = SimpleImputer(strategy='most_frequent')\n",
        "        data_filled[categorical_cols] = simple_imputer.fit_transform(data_filled[categorical_cols])\n",
        "\n",
        "    return data_filled\n",
        "\n",
        "# Create a copy to avoid modifying the original data in place\n",
        "data_filled = data.copy()\n",
        "\n",
        "# Loop through columns with missing values and apply the simplified filling logic\n",
        "for col in missing_value_proportion.index:\n",
        "    # Add a missing value indicator column before filling\n",
        "    data_filled['missing_' + col] = data_filled[col].isnull().astype(float)\n",
        "\n",
        "# Apply the simplified imputation to the whole dataframe\n",
        "data = fill_missing_values_simplified(data_filled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "J1adANnTW1zr",
        "outputId": "95dbab89-d6c9-4dd4-c9d7-8ec911237d47"
      },
      "outputs": [],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "id": "8jC9KfoaZalV",
        "outputId": "26635b8a-b250-4bad-abb3-9fbdd83a695b"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDrUDUA7Y-2U"
      },
      "source": [
        "### Outlier Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlDLa8C1Y-We"
      },
      "outputs": [],
      "source": [
        "def outlier_detection(data):\n",
        "    \"\"\"\n",
        "    Detect outliers for numerical and categorical features.\n",
        "    Returns a DataFrame with outlier flags (1=outlier, 0=normal) for each method.\n",
        "    \"\"\"\n",
        "    results = data.copy()\n",
        "    outlier_flags = pd.DataFrame(index=results.index)\n",
        "\n",
        "    # Numerical: IQR method (threshold=3, typical for moderate outlier frequency)\n",
        "    num_cols = results.select_dtypes(include=[\"number\"]).columns\n",
        "    # Exclude columns that start with 'missing_'\n",
        "    num_cols = [col for col in num_cols if not col.startswith('missing_')]\n",
        "    def iqr_detector(col, threshold=3):\n",
        "        q1 = col.quantile(0.25)\n",
        "        q3 = col.quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        lower_bound = q1 - threshold * iqr\n",
        "        upper_bound = q3 + threshold * iqr\n",
        "        return ((col < lower_bound) | (col > upper_bound)).astype(int)\n",
        "    for col in num_cols:\n",
        "        outlier_flags[f'iqr_{col}'] = iqr_detector(results[col])\n",
        "\n",
        "    # Categorical: rare category (threshold=0.0005, i.e., <.1% frequency)\n",
        "    cat_cols = results.select_dtypes(include=[\"object\", \"category\"]).columns\n",
        "    def category_outlier_detector(col, threshold=0.0005):\n",
        "        freq = col.value_counts(normalize=True)\n",
        "        rare_categories = freq[freq < threshold].index\n",
        "        return col.isin(rare_categories).astype(int)\n",
        "    for col in cat_cols:\n",
        "        outlier_flags[f'cat_outlier_{col}'] = category_outlier_detector(results[col])\n",
        "\n",
        "    return outlier_flags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "demo = outlier_detection(data.copy())\n",
        "demo.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "count = 0\n",
        "for row in demo.index:\n",
        "    if demo.loc[row].sum() > 6:\n",
        "        # print(f\"Row {row} is an outlier in the following methods: {demo.loc[row][demo.loc[row] == 1].index.tolist()}\")\n",
        "        pass\n",
        "    else:\n",
        "        count += 1\n",
        "count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# delete outliers\n",
        "def remove_outliers(data):\n",
        "    \"\"\"\n",
        "    Remove outliers from the DataFrame.\n",
        "    Returns a DataFrame with outliers removed.\n",
        "    \"\"\"\n",
        "    crit = outlier_detection(data)\n",
        "    for row in crit.index:\n",
        "        if crit.loc[row].sum() > 6:\n",
        "            data = data.drop(row)\n",
        "    return data\n",
        "\n",
        "data_no_outliers = remove_outliers(data.copy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# recode the index\n",
        "data_no_outliers.reset_index(drop=True, inplace=True)\n",
        "data_no_outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = data_no_outliers\n",
        "data[\"Age_Class\"] = pd.cut(data[\"age\"], bins=[0, 40, 65, 79, 130], labels=[\"Young Adult\",\"Adult\", \"Senior\", \"Elderly\"])\n",
        "data[\"Age_Class\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data['Risk'] = data['surv2m'] * data['surv2m'] / (data['surv6m']+1e-3)\n",
        "data['phy_Risk'] = data['prg2m'] * data['prg2m'] / (data['prg6m']+1e-3)\n",
        "data['short_term_diff'] = data['surv2m'] / data['prg2m']\n",
        "data['long_term_diff'] = data['surv6m'] / data['prg6m']\n",
        "# sigmoid scaling, change into [0,1] range and indicate a probability of shouldering a risk for month, which in common sense, is related to sfdm2 or death\n",
        "data['Risk'] = 1 / (1 + np.exp(-data['Risk']))\n",
        "data['Risk'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# costs are often right skewed, so we can use log transformation\n",
        "data['charges_log'] = np.log(data['charges'] + 1e-3)  # Adding a small constant to avoid log(0)\n",
        "data['totcst_log'] = np.log(data['totcst'] + 1e-3)  # Adding a small constant to avoid log(0)\n",
        "data['totcst_log'] = 5*(data['totcst_log']-min(data['totcst_log'])) # shift the minimum value to 0 and increase sparseness\n",
        "data['totmcst_log'] = np.log(data['totmcst'] + 1e-3)  # Adding a small constant to avoid log(0)\n",
        "data['totmcst_log'] = 5*(data['totmcst_log']-min(data['totmcst_log'])) # shift the minimum value to 0 and increase sparseness\n",
        "data[['charges_log', 'totcst_log', 'totmcst_log']].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vital Signs\n",
        "# These are often important indicators of health status and can be used to predict outcomes.\n",
        "data['low_BP'] = data['meanbp'] <= 65\n",
        "data['high_HR'] = data['hrt'] >= 100\n",
        "data['high_resp'] = data['resp'] >= 30\n",
        "data['high_temp'] = data['temp'] >= 38.0\n",
        "data['low_temp'] = data['temp'] <= 36.0\n",
        "data[['low_BP', 'high_HR', 'high_resp', 'high_temp', 'low_temp']].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pafi is a measure of the severity of illness, often used in critical care settings.\n",
        "data['ARDS_severity'] = data['pafi'].apply(lambda x: 'Normal' if x >=300 else ('Mild' if x >= 200 else ('Moderate' if x >= 100 else 'Severe')))\n",
        "data['ARDS_severity'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# arterial ph\n",
        "data['acidosis'] = data['ph'] < 7.35\n",
        "data['alkalosis'] = data['ph'] > 7.45\n",
        "data[['acidosis', 'alkalosis']].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# albumin\n",
        "data['albumin_low'] = data['alb'] < 3.5\n",
        "data['albumin_high'] = data['alb'] > 5.0\n",
        "data[['albumin_low', 'albumin_high']].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bilirubin is a measure of liver function, often used in critical care settings.\n",
        "data['bili_high'] = data['bili'] > 2.0\n",
        "data[['bili_high']].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creatinine is a measure of kidney function, often used in critical care settings.\n",
        "data['creatinine_high'] = data['crea'] > 2\n",
        "data[['creatinine_high']].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# blood urea nitrogen (BUN) is a measure of kidney function, often used in critical care settings.\n",
        "data['bun_high'] = data['bun'] > 20\n",
        "# creatinine ratio is a measure of kidney function, often used in critical care settings.\n",
        "data['creatinine_ratio'] = data['crea'] / (data['bun']+1e-6)  # Adding a small constant to avoid division by zero\n",
        "data[['bun_high']].describe(),data[['creatinine_ratio']].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# urine output is a measure of kidney function, often used in critical care settings.\n",
        "data['urine_output_low'] = data['urine'] < 500\n",
        "data[['urine_output_low']].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# glucose is a measure of blood sugar levels, often used in critical care settings.\n",
        "data['glucose_high'] = data['glucose'] > 200\n",
        "data[['glucose_high']].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# adl scores\n",
        "# 1. disability level\n",
        "data['disability_level'] = data['adlsc'] <4\n",
        "# 2. cognitive issues that leads to over optimism\n",
        "data['cognitive_optimism'] = data['adlp']-data['adls']\n",
        "# scaling to [0,1] range, showcasing a probability of cognitive optimism\n",
        "data['cognitive_optimism'] = 1 / (1 + np.exp(-data['cognitive_optimism']))\n",
        "data[['disability_level']].describe(), data[['cognitive_optimism']].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Organ Failures\n",
        "# Each organ failure indicator is boolean, so sum them to count organ failures per row\n",
        "data['organ_failure'] = (\n",
        "\tdata['low_BP'].astype(int) +\n",
        "\tdata['creatinine_high'].astype(int) +\n",
        "\tdata['urine_output_low'].astype(int) +\n",
        "\tdata['acidosis'].astype(int)\n",
        ")\n",
        "data['organ_failure'] =  1.5 / (1 + np.exp(-data['organ_failure'])) - 0.5*1.5\n",
        "data['organ_failure'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# delete columns with low feature importance\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "def delete_low_importance_features(data, target_cols, threshold=0.01):\n",
        "    \"\"\"\n",
        "    Delete columns with low feature importance based on Random Forest Classifier.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : pd.DataFrame\n",
        "        The input DataFrame containing features and target.\n",
        "    target_cols : list\n",
        "        The names of the target columns.\n",
        "    threshold : float\n",
        "        The minimum feature importance value to retain a feature.\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        The DataFrame with low importance features removed.\n",
        "    \"\"\"\n",
        "    oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "    X = data.drop(columns=target_cols)\n",
        "    y = data[target_cols[0]]  # Use the first target column for feature importance\n",
        "\n",
        "    # Encode categorical columns\n",
        "    X_encoded = X.copy()\n",
        "    cat_cols = X_encoded.select_dtypes(include=['object', 'category']).columns\n",
        "    if len(cat_cols) > 0:\n",
        "        X_encoded[cat_cols] = oe.fit_transform(X_encoded[cat_cols])\n",
        "\n",
        "    # Replace inf/-inf with np.nan, then fill np.nan with column median\n",
        "    X_encoded = X_encoded.replace([np.inf, -np.inf], np.nan)\n",
        "    X_encoded = X_encoded.fillna(X_encoded.median(numeric_only=True))\n",
        "\n",
        "    # Fit Random Forest Classifier\n",
        "    model = RandomForestClassifier(n_estimators=500,criterion='entropy',random_state=42)\n",
        "    model.fit(X_encoded, y)\n",
        "\n",
        "    # Get feature importances\n",
        "    importances = model.feature_importances_\n",
        "\n",
        "    # Select features above the threshold\n",
        "    selected_features = X_encoded.columns[importances > threshold]\n",
        "    \n",
        "    return data[selected_features.tolist() + target_cols], model.feature_importances_\n",
        "# Only drop columns that exist in the DataFrame\n",
        "target_cols = [col for col in ['death', 'sfdm2', 'hospdead'] if col in data_no_outliers.columns]\n",
        "data_no_low_importance, importance = delete_low_importance_features(data_no_outliers, target_cols=target_cols, threshold=0.0025)\n",
        "\n",
        "# plot feature importances for selected features only\n",
        "selected_features = data_no_low_importance.drop(columns=['death']).columns\n",
        "\n",
        "# Only drop columns that exist in the DataFrame for original_features\n",
        "original_features = data_no_outliers.drop(columns=target_cols).columns\n",
        "\n",
        "# Get importances for selected_features by matching their positions in original_features\n",
        "selected_importances = [importance[original_features.get_loc(col)] for col in selected_features if col in original_features]\n",
        "\n",
        "# Ensure selected_features and selected_importances have the same length\n",
        "selected_features_plot = [col for col in selected_features if col in original_features]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(selected_features_plot, selected_importances)\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Feature Importances after Removing Low Importance Features')\n",
        "plt.show()\n",
        "print(f\"Number of features after removing low importance features: {len(selected_features_plot)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_cols = data_no_low_importance.select_dtypes(include=['object', 'category']).columns\n",
        "cat_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataUnderstanding(data_no_low_importance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace inf/-inf with np.nan before imputation\n",
        "data_no_low_importance = data_no_low_importance.replace([np.inf, -np.inf], [1e6,1e-6])\n",
        "data_no_low_importance = fill_missing_values_simplified(data_no_low_importance)\n",
        "data_no_low_importance.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures, OrdinalEncoder, OneHotEncoder\n",
        "NominalEncoder_list = ['dzgroup', 'dzclass','ca', 'dnr','sex','race','Age_Class']\n",
        "map_information = {'nominal':{},'ordinal':{}}\n",
        "# check the number of unique values in each categorical column\n",
        "for col in cat_cols:\n",
        "    print(f\"Number of unique values in {col}: {data_no_low_importance[col].nunique()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure data_no_low_importance is defined\n",
        "if 'data_no_low_importance' not in globals():\n",
        "    # Only drop columns that exist in the DataFrame\n",
        "    target_cols = [col for col in ['death', 'sfdm2', 'hospdead'] if col in data_no_outliers.columns]\n",
        "    data_no_low_importance, importance = delete_low_importance_features(data_no_outliers, target_cols=target_cols, threshold=0.005)\n",
        "\n",
        "# encode nominal encoder list as onehot encoder\n",
        "for col in NominalEncoder_list:\n",
        "    if col in data_no_low_importance.columns:\n",
        "        ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
        "        encoded_col = ohe.fit_transform(data_no_low_importance[[col]])\n",
        "        encoded_col_df = pd.DataFrame(encoded_col, columns=[f\"{col}_{i}\" for i in range(encoded_col.shape[1])], index=data_no_low_importance.index)\n",
        "        data_no_low_importance = pd.concat([data_no_low_importance, encoded_col_df], axis=1)\n",
        "        data_no_low_importance.drop(columns=[col], inplace=True)\n",
        "        # record the mapping information\n",
        "        map_information['nominal'][col] = ohe.categories_[0].tolist()\n",
        "map_information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# encode ordinal encoder list and numeric columns\n",
        "for col in cat_cols:\n",
        "    # Skip columns that have been one-hot encoded and dropped, or are not present\n",
        "    if col in NominalEncoder_list or col not in data_no_low_importance.columns:\n",
        "        continue\n",
        "    if data_no_low_importance[col].nunique() <= 30 and col!=\"scoma\":  # Ordinal encoding for categorical columns (all with <= 30 unique values after manual check)\n",
        "        oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "        data_no_low_importance[col] = oe.fit_transform(data_no_low_importance[[col]])\n",
        "        # record the mapping information as a dictionary: number value -> original value\n",
        "        map_information['ordinal'][col] = {i: v for i, v in enumerate(oe.categories_[0])}\n",
        "    else:\n",
        "        data_no_low_importance[col] = data_no_low_importance[col].astype(float)  # Convert to float since mostly stated as float in the codebook\n",
        "print(map_information)\n",
        "# Only show columns that still exist\n",
        "remaining_cat_cols = [col for col in cat_cols if col in data_no_low_importance.columns]\n",
        "data_no_low_importance[remaining_cat_cols]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix7axdmtTLPS"
      },
      "source": [
        "# Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df=data_no_low_importance\n",
        "# use LDA to pick out important features\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "\n",
        "# Identify the correct target column for LDA\n",
        "target_col = df['sfdm2']\n",
        "\n",
        "# Prepare X and y for LDA\n",
        "df = df.drop(columns=['sfdm2'])\n",
        "X_lda_input = df.select_dtypes('number').copy()\n",
        "y_lda = target_col\n",
        "\n",
        "# LDA: n_components must be <= min(n_features, n_classes - 1)\n",
        "n_classes = len(np.unique(y_lda))\n",
        "n_features = X_lda_input.shape[1]\n",
        "max_components = min(n_features, n_classes - 1)\n",
        "lda = LDA(solver='svd',n_components=max_components)\n",
        "X_lda = lda.fit_transform(X_lda_input, y_lda)\n",
        "# Convert X_lda to DataFrame before concatenation\n",
        "X_lda_df = pd.DataFrame(X_lda)\n",
        "lda_df = pd.concat([X_lda_df], axis=1)\n",
        "lda_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 2D t-SNE plot\n",
        "indexes = df[5000:].index\n",
        "data_1000 = lda_df.iloc[indexes]\n",
        "labels_1000 = target_col.iloc[indexes]\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data_1000)\n",
        "tsne = TSNE(n_components=2, perplexity=170, random_state=3407, max_iter=10000)\n",
        "tsne_result = tsne.fit_transform(scaled_data)\n",
        "tsne_df = pd.DataFrame(data=tsne_result, columns=[\"Dim_1\", \"Dim_2\"])\n",
        "# Convert float labels to int for mapping\n",
        "label_map = map_information['ordinal']['sfdm2']\n",
        "tsne_df[\"label\"] = [label_map.get(int(l), str(l)) for l in labels_1000.values]\n",
        "\n",
        "sns.scatterplot(data=tsne_df, x='Dim_1', y='Dim_2',\n",
        "               hue='label', palette=\"bright\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3D t-SNE plot\n",
        "tsne_3D = TSNE(n_components=3, perplexity=170, random_state=3407, max_iter=10000)\n",
        "tsne_result_3D = tsne_3D.fit_transform(scaled_data)\n",
        "tsne_df_3D = pd.DataFrame(data=tsne_result_3D, columns=[\"Dim_1\", \"Dim_2\", \"Dim_3\"])\n",
        "# Convert float labels to int for mapping\n",
        "tsne_df_3D[\"label\"] = [label_map.get(int(l), str(l)) for l in labels_1000.values]\n",
        "\n",
        "# Map string labels to integers for coloring\n",
        "unique_labels = tsne_df_3D[\"label\"].unique()\n",
        "label_to_int = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "tsne_df_3D[\"label_int\"] = tsne_df_3D[\"label\"].map(label_to_int)\n",
        "\n",
        "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
        "\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Create a ListedColormap for consistent color mapping\n",
        "cmap = plt.get_cmap(\"Set1\", len(unique_labels))\n",
        "norm = BoundaryNorm(range(len(unique_labels) + 1), cmap.N)\n",
        "\n",
        "scatter = ax.scatter(\n",
        "\ttsne_df_3D[\"Dim_1\"], tsne_df_3D[\"Dim_2\"], tsne_df_3D[\"Dim_3\"],\n",
        "\tc=tsne_df_3D[\"label_int\"], cmap=cmap, norm=norm, alpha=1, edgecolors='white'\n",
        ")\n",
        "\n",
        "# Create legend with matching colors\n",
        "handles = [\n",
        "\tplt.Line2D([0], [0], marker='o', color='w', label=label,\n",
        "\t\t\t   markerfacecolor=cmap(idx), markersize=10)\n",
        "\tfor label, idx in label_to_int.items()\n",
        "]\n",
        "ax.legend(handles=handles, title=\"label\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckPTuvtxTORU"
      },
      "outputs": [],
      "source": [
        "# Clustering Analysis\n",
        "df = data_no_low_importance\n",
        "df.to_csv(\"preprocessed_data.csv\",index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGVtUgp8TRID"
      },
      "source": [
        "# Prediction: Training and Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O57G64lgTfJd"
      },
      "source": [
        "# Evaluation and Choice of Prediction Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O31sADgnTjX_"
      },
      "source": [
        "# Open-Ended Exploration"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
